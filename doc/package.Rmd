---
title: "Vignette Rmergetrees package"
author: "Audrey Hulot"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE
)
```

```{r, include = FALSE}
library(knitr)
```


Rmergetrees package provides functions to create a consensus tree out of multiple datasets or trees. The main functions of the package are consensusTree and mergeTrees. consensusTree is a wrapper function that calls mergeTrees if the argument is a list of hclust objects.

# Basics 

```{r}
library(Rmergetrees)
```

```{r}
data("iris")
head(iris)
```

Three methods for creating a consensus clustering are proposed in the package: 

* Direct Clustering: merge all datasets into one and perform hierarchical clustering on the result dataset;
* Average Distance: average the distance matrices and perform hierarchical clustering on the result dissimilarity matrix;
* Merge Trees: merge the trees according to a divisive method, and return the consensus tree.

Check before using the function: 

*Check that all objects in the list have the same number of rows/leaves.
*Check all of these rows/leaves are in the same order. For a list of trees, that means the trees are built on data ordered in the same way. 


For better understanding of the methods, we will select 20 individuals in the data
```{r}
set.seed(2000)
iris = iris[sample(1:nrow(iris), 20, replace = FALSE),]
rownames(iris) = 1:nrow(iris)
```


Let's create two datasets out of the iris data.
```{r}
data_list = list("Sepal" = iris[,grep("Sepal", colnames(iris))], "Petal" = iris[,grep("Petal", colnames(iris))])
```

```{r}
library(dendextend) # labels_colors for highlighting groups in a dendrogram
```

Looking at the individual trees for the two datasets:

```{r, fig.align = "center", fig.width = 7, fig.height = 4}
group_color = c("setosa" = "blue", "versicolor" = "black", "virginica" = "red")
par(mfrow = c(1,2), mar = c(2,2,2,0))
res = lapply(1:length(data_list), FUN = function(i){
  hc = hclust(dist(data_list[[i]], method = "euclidean"), method = "ward.D2")
  dend = as.dendrogram(hc)
  labels_colors(dend) = group_color[iris$Species][order.dendrogram(dend)]
  plot(dend, main = names(data_list)[i])
})
```

Creating consensus tree with the three methods in the package:

```{r}
DC = consensusTree(lis = data_list, method = "directClustering", data_st = TRUE, distance_method = "euclidean", linkage_method = "ward.D2", verbose = TRUE)
AD = consensusTree(lis = data_list, method = "averageDistance", dist_st = TRUE, distance_method = "euclidean", linkage_method = "ward.D2", verbose = TRUE)
MT = consensusTree(lis = data_list, method = "mergeTrees", dist_st = TRUE, distance_method = "euclidean", linkage_method = "ward.D2", verbose = TRUE)
```

```{r, fig.align = "center", fig.width = 9, fig.height = 4}
group_color = c("setosa" = "blue", "versicolor" = "black", "virginica" = "red")
list_res = list("DirectClustering" = DC, "AverageDistance" = AD, "MergeTrees" = MT)
par(mfrow = c(1,3), mar = c(2,2,2,0))
res = lapply(1:length(list_res), FUN = function(i){
  hc = list_res[[i]]
  dend = as.dendrogram(hc)
  labels_colors(dend) = group_color[iris$Species][order.dendrogram(dend)]
  plot(dend, main = names(list_res)[i])
})
```

# More details about the mergeTrees method

Consider $\mathcal{T}$ a set of tree with $n$ leaves, and let $\mathcal{C}(\mathcal{T})$ be the consensus tree, then the method can be summarized by this property: 

* For any observations *i* and *j* in $\{1,...,n\}$, $i \neq j$, if they are not in the same cluster in at least one of the trees of $\mathcal{T}$ at height $\lambda$, then they are not in the same cluster in $\mathcal{C}(\mathcal{T})$ at height $\lambda$.

or, equivalently:

* For any observations $i$ and $j$ in $\{1,...,n\}$, $i \neq j$, if they are in the same cluster in all of the trees of $\mathcal{T}$ at height $\lambda$, then they are in the same cluster in $\mathcal{C}(\mathcal{T})$ at height $\lambda$.

The tree aggregation method presented here is a divisive clustering method: as opposed to the agglomerative clustering method, which consider the individuals being in their own groups at the first step and then agglomerate the closest at each step based on an agglomerative criterion, the divisive clustering method starts with all the individuals in the same group and divides the groups based on a divisive criterion.

A tree is considered as a succession of $n-1$ splits, all characterized by the height of the division, and the two clusters created by the division.
Each tree is converted into a list of splits in the beginning of the procedure, leading to a list of $(n-1)\times d$ possible splits to consider to build the consensus tree.
The list of splits is ordered by decreasing height.

At each iteration, the algorithm check if the current split of the list is active or not.

A split is considered active if it impacts the current clustering structure of the data, i.e if at least one element is splitting from its present group.
The consensus tree is built with the active splits. 

This makes the first split always active. 

```{r, out.width = "80%", out.height = "50%", fig.align = "center"}
knitr::include_graphics("Tree_aggregation_description.png")
```

The previous plot shows an example of how the consensus tree is built. The red dashed lines represent the active splits. This table summarizes the possible splits that come frome the two trees, and the splits that are ACTIVE. 

|$~$ | Tree | Split in tree | Height | Cluster 1 | Cluster 2 | Active |
|----:|------:|---------------:|--------:|-----------:|-----------:|--------:|
|  1 | 2 | 1 | 16   | 1    | 2, 3, 4, 5 | ACTIVE |
| 2 | 1 | 1 | 15.3 | 3, 4 | 1, 2, 5    | ACTIVE |
| 3 | 1 | 2 | 10   | 5    | 1, 2       | ACTIVE |
| 4 | 2 | 2 | 9    | 3    | 2, 4, 5    | ACTIVE |
| 5 | 2 | 3 | 6    | 2    | 4, 5       | INACTIVE | 
| 6 | 1 | 3 | 5    | 3    | 5          | INACTIVE |
| 7 | 1 | 4 | 2    | 1    | 2          | INACTIVE |
| 8 | 2 | 4 | 1    | 1    | 4          | INACTIVE |


# Fast hierarchical clustering for high-dimensional data, using convex clustering

merge_trees() is used in the fusedanova package to merge multiple trees obtained with l1-convex clustering method. 

```{r}
devtools::install_github("jchiquet/fusedanova")
```

```{r}
library(fusedanova)
```

Chiquet J, Gutierrez P, Rigaill G: Fast tree inference with weighted fusion penalties, Journal of Computational and Graphical Statistics 205â€“216, 2017. PDF version


# SessionInfo

```{r}
sessionInfo()
```

